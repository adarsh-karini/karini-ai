---
type: "announcement"
show: true
title: "Karini AI enhances AI safety with support for Guardrails"
SEO_title: "Karini AI enhances AI safety with Guardrails for GenAI"
date: "2024-07-17"
time_to_read: "4 min read"
blog_image: "https://d189ftywc9pie0.cloudfront.net/assets/images/blogs/Support_AI_Safety_with_Guardrails.png"
blog_image_alt_name: "Support_AI_Safety_with_Guardrails"
authors:
  - name: "Deepali Rajale"
    image: "https://d189ftywc9pie0.cloudfront.net/assets/images/team_members/deepali-rajale.png"
    linked_in: "https://www.linkedin.com/in/deepali-rajale-958a267/"
SEO_data:
  metadata:
    title: "Elevate AI Trust and Safety with Karini AI & Amazon Bedrock Guardrails"
    keywords: "Karini AI safety guardrails, AI content safety, Amazon Bedrock integration, ethical AI applications, AI risk management"
    description: "Karini AI makes building safe and effective AI easy. Our interface lets you set up guardrails alongside everything else you need, so you can quickly develop responsible AI apps."
    og:local: "en_US"
    og:type: "article"
    og:title: "Elevate AI Trust and Safety with Karini AI & Amazon Bedrock Guardrails"
    og:description: "Karini AI makes building safe and effective AI easy. Our interface lets you set up guardrails alongside everything else you need, so you can quickly develop responsible AI apps."
    og:url: "https://www.karini.ai/announcements/bedrock-guardrails"
    og:site_name: "Karini AI enhances AI safety with support for Guardrails"
    article:published_time: "2024-07-17"
    og:updated_time: "2024-07-17"
    og:image: "https://d189ftywc9pie0.cloudfront.net/assets/images/blogs/Support_AI_Safety_with_Guardrails.png&w=640&q=75"
    og:image:secure_url: "https://d189ftywc9pie0.cloudfront.net/assets/images/blogs/Support_AI_Safety_with_Guardrails.png&w=640&q=75"
    og:image:width: "640"
    og:image:height: "640"
    og:image:alt: "Karini AI enhances AI safety with support for Guardrails"
    twitter:card: "summary"
    twitter:description: "Karini AI makes building safe and effective AI easy. Our interface lets you set up guardrails alongside everything else you need, so you can quickly develop responsible AI apps."
    twitter:title: "Elevate AI Trust and Safety with Karini AI & Amazon Bedrock Guardrails"
    twitter:site: "https://www.karini.ai/announcements/bedrock-guardrails"
    twitter:image: "https://d189ftywc9pie0.cloudfront.net/assets/images/blogs/Support_AI_Safety_with_Guardrails.png&w=640&q=75"
    twitter:creator: "Efficient Batch Pipelines with Karini AI's No-Code Generative AI Recipes"
  canonicalLink: "https://www.karini.ai/announcements/bedrock-guardrails"
  hreflang: "https://www.karini.ai/announcements/bedrock-guardrails"
  schemaMarkup:
    "@context": "https://schema.org"
    "@type": "NewsArticle"
    mainEntityOfPage:
      "@type": "WebPage"
      "@id": "https://www.karini.ai/announcements/bedrock-guardrails"
    headline: "Elevate AI Trust and Safety with Karini AI & Amazon Bedrock Guardrails"
    image: "https://d189ftywc9pie0.cloudfront.net/assets/images/blogs/Support_AI_Safety_with_Guardrails.png"
    author:
      "@type": "Person"
      name: "Deepali Rajale"
    publisher:
      "@type": "Organization"
      name: "Karini AI"
      logo:
        "@type": "ImageObject"
        url: "https://www.karini.ai"
    datePublished: "2024-07-17"
related_posts:
  - "karini-ai-launches-streaming-for-copilot"
  - "karini-ai-unveils-enhanced-prompt-playground"
  - "generative-bi-system"
sitemap:
  loc: "https://www.karini.ai/announcements/bedrock-guardrails"
  lastmod: "2024-07-17"
  changefreq: "monthly"
  priority: "0.9"
  images:
    - loc: "https://d189ftywc9pie0.cloudfront.net/assets/images/blogs/Support_AI_Safety_with_Guardrails.png"
news_sitemap:
  loc: "https://www.karini.ai/announcements/bedrock-guardrails"
  news:
    publication:
      name: "Announcement five"
      language: "en"
    publication_date: "2024-07-17"
    title: "Enhancing AI Safety with Karini Guardrails on AWS"
    keywords: "Karini AI safety guardrails, AI content safety, Amazon Bedrock integration, ethical AI applications, AI risk management"
  images:
    - loc: "https://d189ftywc9pie0.cloudfront.net/assets/images/blogs/Support_AI_Safety_with_Guardrails.png"
---

The rapid advancement of large language models (LLMs) has opened up incredible possibilities in various fields, from enhancing customer service through chatbots to generating creative content and assisting in virtual tasks. These AI-powered applications mimic human-like conversations and can engage users in unprecedented ways. However, it's crucial to be aware that the potential for misuse and harm is significant without proper safeguards. By their nature, LLMs can inadvertently produce and spread misinformation, manipulate individuals, and generate harmful or biased content. This risk underscores the critical need for content safety guardrails to ensure that the benefits of LLMs are realized without compromising ethical standards and societal well-being. This includes the following:

1. **Mitigating Misinformation and Manipulation:** LLMs in chatbots can unintentionally spread misinformation without content safety guardrails, especially in critical areas like healthcare and finance. Guardrails ensure information accuracy, protecting users from being misled.
2. **Preventing Harmful and Biased Outputs:** LLMs may produce harmful or biased content, leading to offensive language or discrimination. Here, content safety guardrails play a crucial role. They filter such outputs and ensure AI generates inclusive and respectful content, providing the audience a sense of reassurance and security.
3. **Enhancing User Trust and Safety:** LLM chatbots may generate offensive or false responses without safeguards. Content safety guardrails build user trust by ensuring safer interactions, particularly in applications for children or vulnerable populations.
4. **Protecting Your Brand and Business:** Chatbot conversations can stray into irrelevant or controversial topics, harming your brand. Guardrails prevent security risks by ensuring LLMs don't expose confidential information or produce harmful outputs.
5. **Facilitating Regulatory Compliance:** Regulatory bodies scrutinize AI for content safety and user protection. Implementing guardrails helps developers comply with regulations, avoid legal issues, and promote sustainable AI deployment.
6. **Ensuring Ethical Use of AI:** Content safety guardrails monitor and control generated content, providing a safer user experience. This is especially crucial for applications targeting children and vulnerable groups. It assures users of the technology's reliability and encourages a responsible and considerate approach to AI use.

Karini AI is proud to announce its support for content safety Guardrails by integrating with [Amazon Bedrock Guardrails](https://aws.amazon.com/bedrock/guardrails/). This integration allows you to configure different filtering policies to avoid undesirable and harmful content and remove or mask sensitive information for privacy protection. Amazon Bedrock Guardrails provide a robust framework for implementing these guardrails, ensuring AI's safety and ethical use in your applications.

- With Karini AI's content filters, you can set thresholds to block input prompts or model responses containing harmful content such as hate speech, insults, sexual content, violence, criminal activity, and prompt attacks, which can include attempts to elicit sensitive information or incite harmful behavior. This feature ensures that your AI applications maintain a safe and respectful environment for users.
- Denied topics: You can set issues to avoid in your generative AI application. For instance, a banking assistant can be programmed to steer clear of illegal investment advice.
- Word filters: You can set custom words or phrases to detect and block user interactions with generative AI applications. For instance, you can filter out profanity and specific terms like competitor names or offensive words.
- Sensitive information filters: You can reject or redact sensitive information in responses depending on the use case. For instance, you can redact personal details while summarizing customer and agent conversation transcripts.

<iframe width="700" height="455" src="https://www.youtube.com/embed/SM23u0W_Gao?si=RnrplOcUN_nW1oZx" title="AI safety with support for Guardrails" frameborder="0" allowfullscreen></iframe>

With Karini AI, you now have:

- An easy and agile interface that allows you to configure your guardrails on the same platform as your prompts, recipes, and copilots.
- This is a flexible way to configure global guardrails at the organization level, which can be applied to all prompts within the organization.
- The ability to Deploy your guardrail as an Amazon Bedrock guardrail.
- Monitor the cost of Guardrails in an application using cost dashboards.

Using the prompt playground, Karini provides a place to incorporate guardrails in prompts and tests. The prompt guardrails are enforced in the associated recipes and copilots, ensuring that your AI applications adhere to the highest safety standards. The prompt playground feature allows you to test and refine your prompts in a controlled environment, ensuring that they meet your content safety requirements before deployment.

Organizations can mitigate risks associated with generative AI applications by incorporating robust safety measures through guardrails, fostering trust and reliability. As these technologies continue to evolve, the emphasis on ethical deployment and rigorous safety protocols will be paramount in leveraging the transformative potential of LLMs while safeguarding users and maintaining public trust. With Karini AI, you can develop innovative and responsible AI applications by leveraging customizable controls tailored to your specific use cases and ethical AI policies. Thus ensuring a balance between technological progress and risk management.
